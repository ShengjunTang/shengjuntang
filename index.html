<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-RJFVJNLM3P"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-RJFVJNLM3P');
</script>
  <title>汤圣君</title>
  <meta name="author" content="Shengjun Tang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
        <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="shortcut icon" href="d.ico">
        <link href="css/bootstrap.min.css" rel="stylesheet" />
	<link href="css/font-awesome.min.css" rel="stylesheet" type="text/css" />
	<link href="https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800" rel="stylesheet" type="text/css" />
	<link href="https://fonts.googleapis.com/css?family=Merriweather:400,300,300italic,400italic,700,700italic,900,900italic" rel="stylesheet" type="text/css" /><!-- Plugin CSS -->
	<link href="css/magnific-popup.css" rel="stylesheet" />
	</head>
<body id="page-top">
	<script>
			  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
			  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
			  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
			  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

			  ga('create', 'UA-59354848-1', 'auto');
			  ga('send', 'pageview');
	</script>

	<script language="javascript" type="text/javascript">
		function showHide(shID) {
		   if (document.getElementById(shID)) {
		      if (document.getElementById(shID+'-show').style.display != 'none') {
		         document.getElementById(shID+'-show').style.display = 'none';
		         document.getElementById(shID).style.display = 'block';
		      }
		      else {
		         document.getElementById(shID+'-show').style.display = 'inline';
		         document.getElementById(shID).style.display = 'none';
		      }
		   }
		}
	</script>
	<nav class="navbar navbar-default navbar-fixed-top" id="mainNav">
		<div class="container-fluid">
			<div class="navbar-header">
				<button class="navbar-toggle collapsed" data-target="#bs-example-navbar-collapse-1" data-toggle="collapse" type="button"><span class="sr-only">Toggle navigation</span> Menu</button><a class="navbar-brand page-scroll" href="#page-top" style="color: black;text-decoration: none;font-size:16pt">Homepage</a>
			</div>
			<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
				<ul class="nav navbar-nav navbar-top">
					<li><a class="page-scroll" href="#about" style="color: black;text-decoration: none;">About</a></li>
					<li><a class="page-scroll" href="#news" style="color: black;text-decoration: none;">News</a></li>
					<li><a class="page-scroll" href="#publications" style="color: black;text-decoration: none;">Publications</a></li>
					<li><a class="page-scroll" href="#researchprojects" style="color: black;text-decoration: none;">Research Projects</a></li>
					<li><a class="page-scroll" href="#awards" style="color: black;text-decoration: none;">Awards</a></li>
					<li><a class="page-scroll" href="#academicservices" style="color: black;text-decoration: none;">Academic Service</a></li>
					<li><a class="page-scroll" href="#activites" style="color: black;text-decoration: none;">Academic Activites</a></li>
				</ul>
			</div>
		</div>
	</nav><br/>
<section id="about">
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>汤圣君 (Tang Shengjun)</name>
              </p>
           <p style="text-align:justify; text-justify:inter-ideograph;">
	    <strong>🌍 <a href="https://geospatial.szu.edu.cn/info/1006/3518.htm">Dr. Shengjun Tang</a> | Researcher & Associate Professor</strong><br/>
	    📍 <strong>Affiliation:</strong> <a href="https://geospatial.szu.edu.cn/kjry/jsxl/2.htm">Shenzhen University</a><br/>
	    🎓 <strong>Education:</strong> Bachelor's & Ph.D. from <a href="http://liesmars.whu.edu.cn/">Wuhan University</a><br/>
	     </p>
	
	<p style="text-align:justify; text-justify:inter-ideograph;">
	    🔬 <strong>Research Experience:</strong><br/>
	    - 🏢 <em>2014-2017:</em> Research Assistant at <a href="https://www.polyu.edu.hk/lsgi/">The Hong Kong Polytechnic University (LSGI)</a><br/>
	    - 🎓 <em>2017-Present:</em> Postdoctoral Researcher → Associate Professor at <a href="https://geospatial.szu.edu.cn/kjry/jsxl/2.htm">Shenzhen University</a>, under the mentorship of <strong>Professor Renzhong Guo</strong><br/>
	</p>
               <p style="text-align:justify; text-justify:inter-ideograph;" >
		       🌟 Welcome to Apply for My Graduate Program! 🌟 <br>
			🚀 Research Areas: <br>
			✅ Urban 3D Reconstruction | ✅  Point Cloud Understanding | ✅ Laser & Visual SLAM <br>
			💡 Who We Are Looking For: <br>
			🎯 Passionate about Multimodal Learning, Deep Learning, and Remote Sensing & Photogrammetry <br>
			🎯 Eager to push the boundaries of academic research & real-world applications <br>
			📢 Join Us at Shenzhen University and explore the limitless possibilities of 3D Space! 🚀🎓 <br>
		        📩 <strong>Get in Touch:</strong> <a href="mailto:shengjuntang@szu.edu.cn">shengjuntang@szu.edu.cn</a><br/>
		 </p>
		       <p style="text-align:center">
                <a href="mailto:shengjuntang@szu.edu.cn">Email</a> &nbsp/&nbsp
                <a href="data/cv_tsj">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=q-fQQqUAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/ShengjunTang">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="Shengjun%20tang.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="shengjuntangcircle1.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
 </section>
<section id="news">
       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <ul>
		<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">12/2024: Congratulations, a paper titled "BIM-Based Indoor Navigation Using End-to-End Visual Localization and ARCore" has been accepted in Transactions In Gis.</li>	
		<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">12/2024: Congratulations! Tang Shengjun has been invited to serve as a member of the editorial board for the journal Photogrammetric Engineering and Remote Sensing(PE&RS) of the American Society for Photogrammetry and Remote Sensing (ASPRS).</li>
		<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2024: Congratulations, a paper titled "Back to Geometry: Efficient Indoor Space Segmentation from Point Clouds by 2D-3D Geometry Constrains" has been accepted in International Journal of Applied Earth Observation and Geoinformation</li>
		<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">08/2024: 祝贺，汤圣君老师获得国家自然科学基金（面上项目）的资助，项目题目为“多模态特征增强的室内点云三维场景图智能构建方法”。（Congratulations to Tang Shengjun on receiving funding from the National Natural Science Foundation of China (General Program) for the project titled "Multimodal Feature-Enhanced Intelligent Construction of Indoor 3D Scene Graphs from Point Clouds.）</li>	
		<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">07/2024: 祝贺，团队论文“面向室内空间智能的三维场景图表达与应用”被测绘学科顶级中文期刊《测绘学报》接收。（Congratulations, the team’s paper titled “Three-Dimensional Scene Graph Representation and Application for Intelligent Indoor Spaces” was accepted by the top-tier Chinese journal in surveying science and technology, Journal of Geodesy and Geomatics.）</li>	
		<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2024: 祝贺，团队论文“BavsNeRF: Batch Active View Selection for Neural Radiance Field Using Scene Uncertainty”被CVPR Workshop条件接收。（Congratulations, the team's paper "BavsNeRF: Batch Active View Selection for Neural Radiance Field Using Scene Uncertainty" has been accepted to The First Workshop on Populating Empty Cities -- Virtual Humans for Robotics and Autonomous Driving at CVPR 2024.）</li>	
		<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2024: Congratulations, Tang Shengjun's paper wins the Best Paper Award for the paper "Accurate Segmentation Method for Roadside Lampposts Based on Vehicle Mounted Lidar Point Clouds" at the ISPRS International Society for Photogrammetry and Remote Sensing Conference. </li>
		<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">05/2024: Congratulations, a paper titled "TreeNet3D: A Large Scale Tree Benchmark for 3D Tree Modeling, Carbon Storage Estimation and Tree Segmentation" has been accepted in International Journal of Applied Earth Observation and Geoinformation</li>
		<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2024: 祝贺，汤圣君老师发表在《武汉大学学报(信息科学版)》论文《超体素随机森林与LSTM神经网络联合优化的室内点云高精度分类方法》，入选2023年度优秀论文。(Congratulations to Shengjun Tang on the publication of his paper "A High-Precision Indoor Point Cloud Classification Method Jointly Optimized by Super Voxel Random Forest and LSTM Neural Network" in Geomatics and Information Science of Wuhan University, which was selected as an outstanding paper of the year 2023.)</li>	
		<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">04/2024: 祝贺，团队论文“AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile Platform Real-Time RGB-D Semantic Segmentation”被CVPR Workshop条件接收。（Congratulations, the team's paper "AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile Platform Real-Time RGB-D Semantic Segmentation" has been conditionally accepted to the CVPR Workshop.）</li>	
		<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">02/2024: 祝贺，汤圣君老师获得亚热带建筑与城市科学全国重点实验室自主研究课题支持，项目题目为“城市卫星拒止环境车载系统高精度定位与三维测图研究”。（ Congratulations to Tang Shengjun for receiving support from State Key Laboratory of Subtropical Building and Urban Science for his independent research project titled "Research on High Precision Positioning and 3D Mapping of Urban Satellite Jamming Environmental Vehicle Systems".）</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody>
        </table>
</section>
<section id="publications">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/arnavigation.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>BIM-Based Indoor Navigation Using End-to-End Visual Localization and ARCore </papertitle>
              </a>
              <br>
               <strong> Shengjun Tang</strong>, Jiawei Wan, Yusong, Li, Hongsheng Huang, Weixi Wang, Renzhong Guo, Yunjie Zhang
              <br>
              Transactions In Gis, 2025 <strong>(SSCI，中科院3区, IF:2.1)</strong>
              <br>
              <a href="https://kdocs.cn/l/ceT4jYXLWTl2"> Download </a> ｜ <a href="https://onlinelibrary.wiley.com/doi/10.1111/tgis.13298"> Cite this </a>
		  <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper proposes an as-built BIM-based indoor navigation using End-to-End visual localization and ARCore. The experimental results demonstrate that in publicly accessible datasets, the global positioning error consistently remains within the 20-centimeter range, with the majority of these datasets achieving a positioning error of 10 centimeters or less. 
	    </td>
          </tr>
	<tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/OptoViewNeRF.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>OptiViewNeRF: Optimizing 3D Reconstruction via Batch View Selection and Scene Uncertainty in Neural Radiance Fields </papertitle>
              </a>
              <br>
              You Li, Rui Li, Ziwei Li, Renzhong Guo, <strong> Shengjun Tang*</strong>
              <br>
              International Journal of Applied Earth Observation and Geoinformation，2025 <strong>(中科院JCR1区,Top, IF:7.5)</strong>
              <br>
              <a href="https://kdocs.cn/l/chETHL7hD1uw"> Download </a> ｜ <a href="https://www.sciencedirect.com/science/article/pii/S1569843224006642"> Cite this </a>
		  <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> We introduce a new framework, OptiViewNeRF, which leverages scene uncertainty to guide the view selection process. Initially, an uncertainty estimation model of the entire scene is developed based on a preliminary NeRF model. This model then informs the selection of new perception viewpoints using a batch view selection strategy, allowing the entire process to be completed in a single iteration. By selecting viewpoints that provide informative data, this approach improves novel view synthesis results and accurately reconstructs 3D scenes.
	    </td>
          </tr>
	<tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/backtogeometry.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
	    <td width="75%" valign="middle">
              <a href="">
                <papertitle>Back to Geometry: Efficient Indoor Space Segmentation from Point Clouds by 2D-3D Geometry Constrains </papertitle>
              </a>
              <br>
             <strong>Shengjun Tang</strong>, Junjie Huang, Benhe Cai, Han Du, Baoding Zhou, Zhigang Zhao, You Li, Weixi Wang
              <br>
              International Journal of Applied Earth Observation and Geoinformation，2024 <strong>(中科院JCR1区,Top, IF:7.5)</strong>
              <br>
              <a href="https://kdocs.cn/l/crqbzoYQC9eX"> Download </a> ｜ <a href="https://www.sciencedirect.com/science/article/pii/S1569843224006216"> Cite this </a>
		  <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper introduces an efficient indoor space segmentation metnod from point clouds. By shifting the focus back to geometric features, the method aims to demonstrate the effectiveness and generalization capabilities of geometry-based approaches in handling diverse indoor structures.  </p>
            </td>
          </tr>
	<tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/indoorscenegraph.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>面向室内空间智能的三维场景图表达与应用 </papertitle>
              </a>
              <br>
             <strong>汤圣君</strong>, 杜思齐, 王伟玺, 郭仁忠
              <br>
              测绘学报，2024
              <br>
              <a href="https://kdocs.cn/l/chFkqYX57cHs"> Download ｜ <a href="http://xb.chinasmp.com/CN/abstract/abstract13336.shtml"> Cite this </a>
		  <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;">本文提出面向室内空间智能的三维场景图表达模型，系统性介绍了室内三维场景图在要素层级化组织、几何表达、语义描述、关系描述方法，建立了一个可统一描述室内要素几何、语义及关系的室内三维场景图概念模型。同时，该图模型可以与现有的三维场景表达方法进行融合表达，具有良好的数据兼容性。  </p>
            </td>
          </tr>
	<tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/treenet.jpg" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>TreeNet3D : A large scale tree benchmark for 3D tree modeling, carbon storage estimation and tree segmentation </papertitle>
              </a>
              <br>
             <strong>Shengjun Tang</strong>, Zhuoyu Ao, Yaoyu Li, Hongsheng Huang, Linfu Xie, Ruisheng Wang, Weixi Wang, Renzhong Guo
              <br>
              International Journal of Applied Earth Observation and Geoinformation，2024 <strong>(中科院JCR1区,Top, IF:7.5)</strong>
              <br>
              <a href="https://www.kdocs.cn/l/cpwmJeiPgHch"> Download </a> |  <a href="https://github.com/ao216/TreeNet3D"> Dataset </a> ｜ <a href="https://www.sciencedirect.com/science/article/pii/S1569843224002577"> Cite this </a> | <a href="Institutions_Usage_Map.html"> 🌏</a> 
		  <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper presents a novel fully automated approach for generating structured 3D synthetic tree models, addressing the limitations of existing datasets used in applications like digital twin construction, carbon stock calculation, and environmental assessments. The method allows for the automated creation of a large-scale dataset containing 13,000 tree models of ten common species, each featuring a detailed 3D point cloud with hierarchical structures, precise parameters, and separate branch and leaf information. The datasets are publicly available, accessible via the  <a href="https://github.com/ao216/TreeNet3D ">TreeNet3D Dataset</a>  link.  </p>
            </td>
          </tr>
	<tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/asymformernew.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>AsymFormer: Asymmetrical Cross-Modal Representation Learning for Mobile Platform Real-Time RGB-D Semantic Segmentation </papertitle>
              </a>
              <br>
             Siqi Du, Weixi Wang, Renzhong Guo, Ruisheng Wang, Yibin Tian and <strong>Shengjun Tang*</strong>
              <br>
              CVPR Workshop, 2024
              <br>
              <a href="https://kdocs.cn/l/cdv7gVmsOOFb"> Download </a> |  <a href="https://github.com/Fourier7754/AsymFormer"> Code </a> ｜ <a href="https://arxiv.org/abs/2309.14065"> Cite this </a>
		  <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;">  AsymFormer uses an asymmetrical backbone for multimodal feature extraction, reducing redundant parameters by optimizing computational resource distribution. To fuse asymmetric multimodal features, a Local Attention-Guided Feature Selection (LAFS) module is used to selectively fuse features from different modalities by leveraging their dependencies. Subsequently, a Cross-Modal Attention-Guided Feature Correlation Embedding (CMA) module is introduced to further extract cross-modal representations. </p>
            </td>
          </tr>
	<tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/trasnscnn.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>TransCNNLoc: End-to-End Pixel-level Learning for 2D-to-3D Pose Estimation in Dynamic Indoor Scene </papertitle>
              </a>
              <br>
             <strong>Shengjun Tang</strong>, Yusong Li, Jiawei Wan, You Li, Baoding Zhou, Renzhong Guo, Weixi Wang, and Yuhong Feng
              <br>
              ISPRS Journal of Photogrammetry and Remote Sensing, 2024 <strong>(中科院JCR1区,Top, IF:12.7)</strong>
              <br>
              <a href="https://kdocs.cn/l/cvNY9x7EG7WN"> Download </a> |  <a href="https://github.com/Geelooo/TransCNNloc"> Code </a> ｜ <a href="https://www.sciencedirect.com/science/article/pii/S0924271623003398?dgcid=coauthor"> Cite this </a>
		  <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this paper, we propose the TransCNNLoc framework, which consists of an encoding-decoding network designed to learn more robust image features for camera pose estimation.Experiments were conducted on the publicly available 7scenes dataset as well as a dataset collected under changing lighting conditions and dynamic scenes for accuracy validation and analysis. The experimental results demonstrate that the proposed TransCNNLoc framework exhibits superior adaptability to dynamic scenes and lighting changes. </a>
              </p>
            </td>
		
          </tr>
	<tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/treejag.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>Branching the Limits: Robust 3D Tree Reconstruction from Incomplete Laser Point Clouds</papertitle>
              </a>
              <br>
              Weixi Wang, Yaoyu Li, Hongsheng Huang, Linping Hong, Siqi Du, Linfu Xie, Xiaoming Li, Renzhong Guo and <strong>Shengjun Tang*</strong>
              <br>
              International Journal of Applied Earth Observation and Geoinformation, 2023 <strong>(中科院JCR1区,Top, IF:7.5)</strong>
              <br>
              <a href="https://kdocs.cn/l/cf1uuucdPkc5"> Download </a> ｜ <a href="https://www.sciencedirect.com/science/article/pii/S1569843223003813"> Cite this </a> 
		  <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this paper, a novel method for fine-grained 3D tree model reconstruction from incomplete point clouds is proposed. This approach addresses the structural reconstruction of trees, tackling two main problems: reconstructing the main trunk and branches based on data completeness.</a>
              </p>
            </td>
          </tr>
	<tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/bplidar.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="">
                <papertitle>Backpack LiDAR-based SLAM with Multiple Ground Constraints for Multi-storey Indoor Mapping</papertitle>
              </a>
              <br>
              Baoding Zhou, Haoquan Mo, <strong>Shengjun Tang*</strong>, Xing Zhang, and Qingquan Li
              <br>
              IEEE Transactions on Geoscience and Remote Sensing, 2023 <strong>(中科院JCR1区,Top, IF:8.2)</strong>
              <br>
              <a href="https://kdocs.cn/l/ckw8aQYAxZzw"> Download </a> ｜ <a href="https://ieeexplore.ieee.org/abstract/document/10318824"> Cite this </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper proposed a SLAM method base on multiple ground constraints pose optimization (MGCPO) which uses a backpack LiDAR system. The proposed method includes two novel modules. The first, a regression analysis-based scenarios recognition (RASR) module provides a reference for the construction of ground constraints. The second, based on different scene detection results, the MGCPO module constrains the sensor pose using the floor plane to reduce localization errors and effectively decrease loop closure detection errors.</a>
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/calib.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EeEA--97IxxGjppxKUhz1j8B36Z6Gn8V8JXrEu4HUzf_Wg?e=nq5M6M">
                <papertitle>Robust Calibration of Vehicle Solid-state Lidar-Camera Perception System Using Line-Weighted Correspondences in Natural Environments</papertitle>
              </a>
              <br>
              <strong>Shengjun Tang</strong>, Yunqi Feng, Junjie Huang, Xiaoming Li, Zhihan Lv, Yuhong Feng, and Weixi Wang
              <br>
              IEEE Transactions on Intelligent Transportation Systems, 2023 <strong>(中科院JCR1区, Top, IF:8.5)</strong>
              <br>
              <a href="https://kdocs.cn/l/cvjh0UQ078y6"> Download </a> ｜ <a href="https://ieeexplore.ieee.org/document/10315057"> Cite this </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this paper, we present a novel approach for robustly calibrating the extrinsic parameters of a solid-state(SS) lidar-camera system in a natural environment.  We conducted a performance study to compare our proposed method against existing targetless calibration methods on various natural scenarios. The experimental results demonstrate that our proposed method achieves higher robustness, accuracy, and consistency, making it suitable for real-world applications.</a>
              </p>
            </td>
          </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Syndatasets_AIC2023.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EeEA--97IxxGjppxKUhz1j8B36Z6Gn8V8JXrEu4HUzf_Wg?e=nq5M6M">
                <papertitle>Skeleton-guided generation of synthetic noisy point clouds from as-built BIM to improve indoor scene understanding</papertitle>
              </a>
              <br>
              <strong>Shengjun Tang</strong>, Hongsheng Huang, Yunjie Zhang, Mengmeng Yao, Xiaoming Li, Linfu Xie, and Weixi Wang
              <br>
              Automation in Construction, 2023 <strong>(中科院JCR1区, Top, IF:10.3)</strong>
              <br>
              <a href="https://kdocs.cn/l/com90I4GfXzv"> Download </a> | <a href="https://szueducn-my.sharepoint.com/:f:/g/personal/shengjuntang_szu_edu_cn/EmAdsrCIxrtEl21hocr8NMwB11DDjCJJM9ATA8Yzt1uNLQ?e=Ze0r5j"> Datasets </a> ｜ <a href="https://www.sciencedirect.com/science/article/pii/S0926580523003369"> Cite this </a>
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this study, a fully automatic method to generate synthetic noisy point clouds from as-built building information modeling (BIM) models is presented and it assesses the potential of these synthetic point clouds to improve deep neural network training. All simulation datasets are publicly available, including original BIM models, full synthetic point clouds, and point clouds after IHPR processing, accessible via the following link: <a href="https://szueducn-my.sharepoint.com/:f:/g/personal/shengjuntang_szu_edu_cn/EgpOf3leEiFOjtfQTl-k7GgBougQGuQFcHDAbgtBZmVZ5w?e=rPPlot"> BIMSyn Dataset.</a>
              </p>
            </td>
          </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/treemodeling.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.jors.cn/jrs/ch/reader/download_new_edit_content.aspx?file_no=202210300000001&flag=2">
                <papertitle>面向虚拟地理环境构建的树木模型高保真三维重建方法(A highly realistic 3D reconstruction method for tree models created for virtual geographic environments) </papertitle>
              </a>
              <br>
              王伟玺(Weixi Wang), 黄鸿盛(Hongsheng Huang),杜思齐(Siqi Du), 李晓明(Xiaoming Li), 谢林甫(Linfu Xie), 郭仁忠(Renzhong Guo), <strong>汤圣君(Shengjun Tang)*</strong>
              <br>
              遥感学报(Journal of Remote Sensing), 2023
              <br>
              <a href="http://www.jors.cn/jrs/ch/reader/download_new_edit_content.aspx?file_no=202210300000001&flag=2"> Download </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> 本文面向虚拟地理环境高逼真场景构建需求，提出一种基于高精度激光扫描点云数据的树木三维模型高保真仿生重建方法，可实现形态特征保持的树木三维模型自动化重建。(In this paper, we propose a bionic reconstruction method for 3D tree models based on high-precision laser scanning point cloud data for building realistic scenes in virtual geographic environments, which enables the automated reconstruction of 3D tree models at multiple levels of detail while preserving morphological features.)     
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/indoorclassification.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://ch.whu.edu.cn/article/exportPdf?id=dbb0171b-533f-4d8a-a3d7-67e694032f82">
                <papertitle>超体素随机森林与 LSTM 神经网络联合优化的室内点云高精度分类方法(A High-Precision Indoor Point Cloud Classification Method Jointly Optimized by Super Voxel Random Forest and LSTM Neural Network) </papertitle>
              </a>
              <br>
              <strong>汤圣君(Shengjun Tang)</strong>, 张韵婕(Yunjie Zhang), 李晓明(Xiaoming Li), 姚萌萌(Mengmeng Yao), 叶致煌(Zhihuang Ye), 李亚鑫(Yaxin Li), 郭仁忠(Renzhong Guo), 王伟玺(Weixi Wang)
              <br>
              武汉大学学报（信息科学版）(Geomatics and Information Science of Wuhan University), 2023
              <br>
              <a href="http://ch.whu.edu.cn/article/exportPdf?id=dbb0171b-533f-4d8a-a3d7-67e694032f82"> Download </a>
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> 针对现有三维点云数据分割分类方法存在分类目标内部不一致的问题，提出一种超体素随机森林与长短期记忆神经网络(long short-term memory，LSTM)联合优化的室内点云高精度分类方法。  (In response to the problem of internal inconsistency in the classification targets of existing 3D point cloud data segmentation methods, we propose a high-precision indoor point cloud classification method that jointly optimizes super-voxel random forests and Long Short-Term Memory (LSTM) neural networks.)   
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/jag_pond.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://pdf.sciencedirectassets.com/272637/1-s2.0-S1569843223X0003X/1-s2.0-S1569843223001486/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEG0aCXVzLWVhc3QtMSJGMEQCIDb2spK%2F67VOb7fKXQu6A28R%2FRfteSf3Hf8U7wPXBjukAiATFZB7Xj9ZbJ8C0NSC%2FGp6SaZ%2F%2BZfomOFtn5MO0%2BCDHCq8BQiG%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMhWzSdAUIL%2BGVk1LjKpAFzpyCoIDb4WysFCxf5mFDpTYMqLaFPMpZ64P62%2BJeWN%2BLT1TpKzLSNRaNgjel0oV%2FrZfMcpRqeo6avPA6Ltcz7ifE1XfTNgG5irB2KXZhMLLqd1PRz%2FrM7U5hSG1Pyp88gL0r1HQccMJnFalpkQbqP7fNdzJ0ZsVpRJv0DP%2BslM9gM6fF5zkD%2FyDv4OfzDQoDESIqpJzlJY%2FarESW7WaL5N0KJ1MgRVck%2FEZ1o1jr4xlorWjeMumuc532mQqPk7mpTKxssy53aUYr0pigHtSlCLNkAZSdQnYqjb7OR%2BFrCkETGG1Lyvu0iW2ULdYOfGMlQezISpA8seINAE1MHvpdZQI%2BAyOFL5uVPWuTelJ7t7GE5ZW%2BO%2F40oHzGfUnM7yVdXZo60CEEXrtv1wFxHhJRIHsei%2FtVqE7RXCYNLJum8M3g85t%2FA4AkpkANmCzsH3HEDfaS5DMH2H7cxTEF4BEapdgvKVCoUkb2rg0A%2BZJvLNpkTUVob9JZRFcnKjZ1oDy5PLZwyS219xA7DxPRcROG1gOPg49SA7DiojyC8frOg6hhl%2FErOcRguHbgaVSC3AY5tyJHGnyLNviad0Fb4MEXlAVzCCwK03VpraBsYr5Oxv9ShZMnLE0OaaClGefDS%2F6ckxV91KYJRYSHs1GUQFQkFKXyoEhp6fjObHRd3U1zfAk3QSB%2FV%2B%2FDxaQb%2FodAVvU3O5aIg67byP3iPDcntxmUVsMsSYDNNLbqltyTEWbVwQCtIkS4te85Oo%2B1ieRuaSLWjA4Q%2F1Oulz3tA5naLvAu3dqLY07%2BI5Ub9csEsWBgM0E5DzfFyUQhip7r3CmQ7dYv4gFToGcknMVQKWtMwBkwkP%2FgoHJ61f6FrEp5maAz6Ccw2b3sogY6sgES391tlNmRCrSzKJVA90AqGJRHt5%2BlTMNeobV7JEtrKCkvwJDERzgT2PPCClAkmOn7dlVU%2BudScI9XCRhB%2FBNLXKeqELbrwtfH7jZMnnJeUExhKLOClOxVOQ7jRY5EARBOCmzuWuh5vg6Q1B4XQS8qZUOoMdtuOTIJ%2B8A%2ByUh0leH9AAabPmmBuqpEFqwevTFzxeT0aDICXucd%2BgcKKhDHZz4c%2B92Y8KAfDOcWCsGZHxyx&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230510T055327Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3JMUZDOO%2F20230510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8ee5e89b67d4c3354e3898e0bcfa81f8c52542ffbd2acc2a441762189f289148&hash=a7a8c4a72931447bc5128a10de27e8eeb9ec0e2a0d54ed10d3d07aabb3f38547&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1569843223001486&tid=spdf-a022235e-841c-42db-a0d0-c7d388617534&sid=db751d415e6a084c0c3a0af-48fdc60f973dgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0c10500155040600545d&rr=7c4fec03db5225e7&cc=jp">
                <papertitle>Unsupervised stepwise extraction of offshore aquaculture ponds using super-resolution hyperspectral images </papertitle>
              </a>
              <br>
	      Siqi Du, Hongsheng Huang, Fan He, Heng Luo, Yumeng Yin, Xiaoming Li, Linfu Xie, Renzhong Guo,  <strong>Shengjun Tang*</strong>
              <br>
              International Journal of Applied Earth Observation and Geoinformation, 2023 <strong>(中科院JCR1区,Top, IF:7.5)</strong>
              <br>
              <a href="https://pdf.sciencedirectassets.com/272637/1-s2.0-S1569843223X0003X/1-s2.0-S1569843223001486/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEG0aCXVzLWVhc3QtMSJGMEQCIDb2spK%2F67VOb7fKXQu6A28R%2FRfteSf3Hf8U7wPXBjukAiATFZB7Xj9ZbJ8C0NSC%2FGp6SaZ%2F%2BZfomOFtn5MO0%2BCDHCq8BQiG%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMhWzSdAUIL%2BGVk1LjKpAFzpyCoIDb4WysFCxf5mFDpTYMqLaFPMpZ64P62%2BJeWN%2BLT1TpKzLSNRaNgjel0oV%2FrZfMcpRqeo6avPA6Ltcz7ifE1XfTNgG5irB2KXZhMLLqd1PRz%2FrM7U5hSG1Pyp88gL0r1HQccMJnFalpkQbqP7fNdzJ0ZsVpRJv0DP%2BslM9gM6fF5zkD%2FyDv4OfzDQoDESIqpJzlJY%2FarESW7WaL5N0KJ1MgRVck%2FEZ1o1jr4xlorWjeMumuc532mQqPk7mpTKxssy53aUYr0pigHtSlCLNkAZSdQnYqjb7OR%2BFrCkETGG1Lyvu0iW2ULdYOfGMlQezISpA8seINAE1MHvpdZQI%2BAyOFL5uVPWuTelJ7t7GE5ZW%2BO%2F40oHzGfUnM7yVdXZo60CEEXrtv1wFxHhJRIHsei%2FtVqE7RXCYNLJum8M3g85t%2FA4AkpkANmCzsH3HEDfaS5DMH2H7cxTEF4BEapdgvKVCoUkb2rg0A%2BZJvLNpkTUVob9JZRFcnKjZ1oDy5PLZwyS219xA7DxPRcROG1gOPg49SA7DiojyC8frOg6hhl%2FErOcRguHbgaVSC3AY5tyJHGnyLNviad0Fb4MEXlAVzCCwK03VpraBsYr5Oxv9ShZMnLE0OaaClGefDS%2F6ckxV91KYJRYSHs1GUQFQkFKXyoEhp6fjObHRd3U1zfAk3QSB%2FV%2B%2FDxaQb%2FodAVvU3O5aIg67byP3iPDcntxmUVsMsSYDNNLbqltyTEWbVwQCtIkS4te85Oo%2B1ieRuaSLWjA4Q%2F1Oulz3tA5naLvAu3dqLY07%2BI5Ub9csEsWBgM0E5DzfFyUQhip7r3CmQ7dYv4gFToGcknMVQKWtMwBkwkP%2FgoHJ61f6FrEp5maAz6Ccw2b3sogY6sgES391tlNmRCrSzKJVA90AqGJRHt5%2BlTMNeobV7JEtrKCkvwJDERzgT2PPCClAkmOn7dlVU%2BudScI9XCRhB%2FBNLXKeqELbrwtfH7jZMnnJeUExhKLOClOxVOQ7jRY5EARBOCmzuWuh5vg6Q1B4XQS8qZUOoMdtuOTIJ%2B8A%2ByUh0leH9AAabPmmBuqpEFqwevTFzxeT0aDICXucd%2BgcKKhDHZz4c%2B92Y8KAfDOcWCsGZHxyx&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230510T055327Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY3JMUZDOO%2F20230510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=8ee5e89b67d4c3354e3898e0bcfa81f8c52542ffbd2acc2a441762189f289148&hash=a7a8c4a72931447bc5128a10de27e8eeb9ec0e2a0d54ed10d3d07aabb3f38547&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1569843223001486&tid=spdf-a022235e-841c-42db-a0d0-c7d388617534&sid=db751d415e6a084c0c3a0af-48fdc60f973dgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0c10500155040600545d&rr=7c4fec03db5225e7&cc=jp"> Download </a> ｜ <a href="https://www.sciencedirect.com/science/article/pii/S1569843223001486"> Cite this </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this paper, we proposed an unsupervised aquaculture ponds extraction method based on hyperspectral imagery super-resolution, feature fusion and stepwise extraction strategy. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/treesegJSTAR.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10040735">
                <papertitle>An Individual Tree Segmentation Method from Mobile Mapping Point Clouds Based on Improved 3D Morphological Analysis</papertitle>
              </a>
              <br>
             Weixi Wang, Yuhang Fan, You Li, Xiaoming Li, <strong>Shengjun Tang*</strong>
              <br>
              IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2023 <strong>(中科院3区, IF:5.5)</strong>
              <br>
              <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10040735"> Download </a>  ｜ <a href="https://ieeexplore.ieee.org/abstract/document/10040735"> Cite this </a>
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> We propose a method based on improved 3D morphological analysis for extracting street trees from mobile laser scanner (MLS) point clouds. 
              </p>
            </td>
          </tr>


        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/AutoBIM_AIC2022.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/Ed6Z8EtiPuJCnmUxBtFyRh4BybZIVqXjb0ooq_NgcrynmA?e=k4vy8o">
                <papertitle>BIM generation from 3D point clouds by combining 3D deep learning and improved morphological approach</papertitle>
              </a>
              <br>
             <strong> Shengjun Tang</strong>, Xiaoming Li, Xianwei Zheng, Bo Wu, Weixi Wang, Yunjie Zhang
              <br>
              Automation In Construction, 2022 <strong>(中科院JCR1区, Top, IF: 10.3)</strong>
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/Ed6Z8EtiPuJCnmUxBtFyRh4BybZIVqXjb0ooq_NgcrynmA?e=k4vy8o"> Download </a>  ｜ <a href="https://www.sciencedirect.com/science/article/pii/S0926580522002953"> Cite this </a>
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper presents a novel parametric modeling method for reconstructing semantic volumetric building interiors from the unstructured point cloud of a building. Unlike existing partitioning-based methods, our proposed method overcomes the limitations by providing a flexible framework for combining 3D Deep Learning and an improved morphological approach for inverse BIM modeling. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/superV_RS2022.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/2072-4292/14/6/1516/pdf">
                <papertitle>A Supervoxel-Based Random Forest Method for Robust and Effective Airborne LiDAR Point Cloud Classification</papertitle>
              </a>
              <br>
             Lingfeng Liao, <strong> Shengjun Tang*</strong>, Jianghai Liao, Xiaoming Li, Weixi Wang, Yaxin Li, Renzhong Guo
              <br>
              Remote Sensing, 2022 <strong>(中科院JCR2区,Top, IF:5.6)</strong>
              <br>
              <a href="https://www.mdpi.com/2072-4292/14/6/1516/pdf"> Download </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this paper, we propose a robust and effective point cloud classification approach that integrates point cloud supervoxels and their locally convex connected patches into a random forest classifier, which effectively improves the point cloud feature calculation accuracy and reduces the computational cost. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/pscnet_isprsannals2022.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://isprs-annals.copernicus.org/articles/V-1-2022/129/2022/isprs-annals-V-1-2022-129-2022.pdf">
                <papertitle>PSCNET: EFFICIENT RGB-D SEMANTIC SEGMENTATION PARALLEL NETWORK BASED ON SPATIAL AND CHANNEL ATTENTION </papertitle>
              </a>
              <br>
              Siqi Du, <strong>Shengjun Tang*</strong>, Weixi Wang, Xiaoming Li, Yonghua Lu, Renzhong Guo
              <br>
              ISPRS Annals, 2022
              <br>
              <a href="https://isprs-annals.copernicus.org/articles/V-1-2022/129/2022/isprs-annals-V-1-2022-129-2022.pdf"> Download </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> RGB-D semantic segmentation algorithm is a key technology for indoor semantic map construction. The traditional RGB-D semantic segmentation network, which always suffer from redundant parameters and modules. In this paper, an improved semantic segmentation network PSCNet is designed to reduce redundant parameters and make models easier to implement. Based on the DeepLabv3+ framework, we have improved the original model in three ways, including attention module selection, backbone simplification, and Atrous Spatial Pyramid Pooling (ASPP) module simplification.
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/indoorSeg_ISPRS2021.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EV-E_9A09QVCiSIps89ER40BwApFmMf3FvPBwBk29_vrrQ?e=2Mg09P">
                <papertitle>Learning deep cross-scale feature propagation for indoor semantic segmentation </papertitle>
              </a>
              <br>
              Linxi Huan, Xianwei Zheng, <strong>Shengjun Tang</strong>, Jianya Gong
              <br>
              ISPRS Journal of Photogrammetry and Remote Sensing, 2021  <strong>(中科院JCR1区, Top, IF: 12.7)</strong>
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EV-E_9A09QVCiSIps89ER40BwApFmMf3FvPBwBk29_vrrQ?e=2Mg09P"> Download </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper proposes a deep cross-scale feature propagation network (CSNet), to effectively learn and fuse multi-scale features for robust semantic segmentation of indoor scene images. The proposed CSNet is deployed as an encoder-decoder engine. During encoding, the CSNet propagates contextual information across scales and learn discriminative multi-scale features, which are robust to large object scale variation and indoor occlusion. The decoder of CSNet then adaptively integrates the multi-scale encoded features with fusion supervision at all scales to generate target semantic segmentation prediction. 
              </p>
            </td>
          </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/FITEE_2021.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EbAatnqld3RAgRJGf-S-f8wBaBLqaPx4gLjfzb_WiXJWhw?e=KK9PNJ">
                <papertitle>A survey on indoor 3D modeling and applications via RGB-D devices </papertitle>
              </a>
              <br>
              Zhilu Yuan, You Li,<strong>Shengjun Tang*</strong>, Ming Li, Renzhong Guo, Weixi Wang
              <br>
              Frontiers of Information Technology & Electronic Engineering, 2021 <strong>(中科院3区, Top, IF: 3)</strong>
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EbAatnqld3RAgRJGf-S-f8wBaBLqaPx4gLjfzb_WiXJWhw?e=KK9PNJ"> Download </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this survey, we provide an overview of recent advances in indoor scene modeling methods, public indoor datasets and libraries which can facilitate experiments and evaluations, and some typical applications using RGB-D devices including indoor localization and emergency evacuation.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/TrSLAM_PERS2020.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ETmbpTQidZVDpsfKVwdpqzAB-ntaPzFp8WRePHon9bboBQ?e=rYFnBl">
                <papertitle>Trajectory Drift–Compensated Solution of a Stereo RGB-D Mapping System </papertitle>
              </a>
              <br>
             <strong>Shengjun Tang</strong>, Qing Zhu, You Li, Wu Chen, Bo Wu, Renzhong Guo, Xiaoming Li, Chisheng Wang, Weixi Wang
              <br>
              Photogrammetric Engineering & Remote Sensing, 2020 <strong>(中科院4区, Top, IF: 1.3)</strong>
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ETmbpTQidZVDpsfKVwdpqzAB-ntaPzFp8WRePHon9bboBQ?e=rYFnBl"> Download </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> In this paper, we describe the trajectory drift–compensated strategy that we designed to eliminate the influence of time drift between sensors, remove the inconsistency between the sequences from various sensors, and thereby generate a coarse-to-fine procedure for robust camera-tracking based on two-dimensional–3D observations from stereo sensors. 
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/polelike_RS2019.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.mdpi.com/2072-4292/11/24/2920/pdf">
                <papertitle>Pole-Like Street Furniture Segmentation and Classification in Mobile LiDAR Data by Integrating Multiple Shape-Descriptor Constraints </papertitle>
              </a>
              <br>
             You Li, Weixi Wang, Xiaoming Li, Linfu Xie, Yankun Wang, Renzhong Guo, Wenqun Xiu, <strong>Shengjun Tang*</strong>
              <br>
              Remote Sensing, 2019 <strong>(中科院JCR2区,Top, IF:5.6)</strong>
              <br>
              <a href="https://www.mdpi.com/2072-4292/11/24/2920/pdf"> Download </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> We present a complete paradigm for pole-like street furniture segmentation and classification using mobile LiDAR (light detection and ranging) point cloud. 
              </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/ComBA_ISPRS2016.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ESkBuqdEjHpMid6GeXsTuzYB70vNeurvdLG7P_-4AjcQeQ?e=gai1V1">
                <papertitle>Combined adjustment of multi-resolution satellite imagery for improved geo-positioning accuracy</papertitle>
              </a>
              <br>
              <strong>Shengjun Tang</strong>, Bo Wu, Qing Zhu
              <br>
              ISPRS Journal of Photogrammetry and Remote Sensing, 2016  <strong>(中科院JCR1区,Top, IF:12.7)</strong>
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/ESkBuqdEjHpMid6GeXsTuzYB70vNeurvdLG7P_-4AjcQeQ?e=gai1V1"> Download </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> This paper presents a combined adjustment approach to integrate multi-source multi-resolution satellite imagery for improved geo-positioning accuracy without the use of ground control points (GCPs). 
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/Com_ISPRS2015.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.lsgi.polyu.edu.hk/staff/bo.wu/publications/Wu%20et%20al.%202015_Geometric%20Integration%20of%20HRSI%20and%20LiDAR%20Data.pdf">
                <papertitle>Geometric integration of high-resolution satellite imagery and airborne LiDAR data for improved geopositioning accuracy in metropolitan areas</papertitle>
              </a>
              <br>
              Bo Wu, <strong>Shengjun Tang</strong>, Qing Zhu, Kwan-yuen Tong, Han Hu, Guoyuan Li
              <br>
              ISPRS Journal of Photogrammetry and Remote Sensing, 2015  <strong>(中科院JCR1区,Top, IF:12.7)</strong>
              <br>
              <a href="http://www.lsgi.polyu.edu.hk/staff/bo.wu/publications/Wu%20et%20al.%202015_Geometric%20Integration%20of%20HRSI%20and%20LiDAR%20Data.pdf"> Download </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;">  Considering HRSI and LiDAR datasets taken from metropolitan areas as a case study, this paper presents a novel approach to the geometric integration of HRSI and LiDAR data to reduce their inconsistencies and improve their geopositioning accuracy. 
              </p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/para2014.png" alt="PontTuset" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EdkBd-LtirBLuFLdy87tv7UBAsyvVgggfexuGrIkwKHr9g?e=7QM6lw	">
                <papertitle>三维 GIS 中的参数化建模方法(parametric modeling method in three-dimensional GIS) </papertitle>
              </a>
              <br>
              <strong>汤圣君(Shengjun Tang)</strong>, 张叶廷(Yeting Zhang), 许伟平(Weiping Xu), 谢潇(Xiao Xie), 朱庆(Qing Zhu), 韩元利(Yuanli Han), 吴强(Qiang Wu)
              <br>
              武汉大学学报（信息科学版）(Geomatics and Information Science of Wuhan University), 2014
              <br>
              <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EdkBd-LtirBLuFLdy87tv7UBAsyvVgggfexuGrIkwKHr9g?e=7QM6lw"> Download </a> 
              <p></p>
              <p style="text-align:justify; text-justify:inter-ideograph;"> 为实现大规模地形景观和精细工程设施模型在三维GIS中的无缝集成管理,并支持工程设施全生命周期的共享应用,提出了一种可根据设计参数自动建立复杂设施三维模型并交互式编辑修改的方法,扩展了三维GIS数据模型,实现了三维几何模型与其参数信息的有机集成与同步更新,并以桥梁模型的构建为例验证了该方法的可行性和有效性。(To realize the seamless integrated management of large-scale terrain landscape and detailed engineering facility models in 3D GIS, and to support the shared application of the whole lifecycle of engineering facilities, a method is proposed. This method can automatically build complex facility 3D models based on design parameters and allows interactive editing and modification. This method extends the 3D GIS data model, achieves the organic integration and synchronous update of the 3D geometric model and its parameter information. The feasibility and effectiveness of this method have been verified using the construction of a bridge model as an example.)
              </p>
            </td>
          </tr>
          <tr>
        </tbody></table>
	</section>
	<section id="researchprojects">
	 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Research projects</heading>
              <p>
                <ul>
		    <li style="text-align:justify; text-justify:inter-ideograph;">国家自然科学基金（面上基金），多模态特征增强的室内点云三维场景图智能构建方法<a href="https://www.kdocs.cn/l/cmkHZrXJcuZ0"> ⏬</a>， 42471442， 2025/01/01 至 2028/12/31, RMB$460,000, 主持</li>
		    <li style="text-align:justify; text-justify:inter-ideograph;">深圳市科技计划-科技重大专项，城市数字孪生三维场景建模关键技术研究及应用示范-课题<a href="https://www.kdocs.cn/l/cr8WkXkatgFc"> ⏬</a>， KJZD20230923115508017， 2024/01/01 至 2026/12/31, RMB$1800,000, 主持</li>
		    <li style="text-align:justify; text-justify:inter-ideograph;">广东省自然科学基金-青年提升项目，空地跨视角协同感知的移动车载平台精准定位与位姿优化方法<a href="https://www.kdocs.cn/l/cv6Rn50mtXsT"> ⏬</a>， 2024A1515030061， 2024/01/01 至 2026/12/31, RMB$300,000, 主持</li>
		    <li style="text-align:justify; text-justify:inter-ideograph;">亚热带建筑与城市科学全国重点实验室，自主研究课题，城市卫星拒止环境车载系统高精度定位与三维测图研究<a href="https://www.kdocs.cn/l/canBKCBPo0Uh"> ⏬</a>, 2023ZB18， 2024/01/01 至 2026/12/31, RMB$30,000,0, 主持</li>
		    <li style="text-align:justify; text-justify:inter-ideograph;">中华人民共和国科学技术部，国家重点研发计划项目子课题，超大城市绿色低碳发展监测与诊断优化应用示范<a href="https://www.kdocs.cn/l/cthnFElhhwNX"> ⏬</a>, 2022YFB3903700， 2023/12 至 2026/11, RMB$58,800,0, 主持</li>
                    <li style="text-align:justify; text-justify:inter-ideograph;">广东省面上基金，联合视觉SLAM与深度神经网络的室内场景语义分类与自动化建模技术<a href="https://www.kdocs.cn/l/cp5wpfdYQWc6"> ⏬</a>，2021A1515012574，2021/01/01 至 2023/12/31，RMB$100,000，主持</li>
                    <li style="text-align:justify; text-justify:inter-ideograph;">深圳市基础研究面上项目，面向机器人自主导航的类脑视觉场景理解与实时位置计算方法<a href="https://www.kdocs.cn/l/cgkTI2HhbCkb"> ⏬</a>，JCYJ20210324093012033，2021/10/28 至 2024/10/27, RMB$600,000，主持</li>
                    <li style="text-align:justify; text-justify:inter-ideograph;">2020年佛山市促进高校科技成果服务产业发展扶持项目，室内全空间三维测图于标准化建模技术研究 <a href="https://kdocs.cn/l/cd58y9IU2mII"> ⏬</a>，2020/10/13 至 2022/12/31,RMB$500,00，主持</li>	
            	    <li style="text-align:justify; text-justify:inter-ideograph;">中交第二航务工程勘察设计院有限公司，雄安新区孝义河倾斜摄影技术服务<a href="https://kdocs.cn/l/ciZQI0tkzL8p"> ⏬</a>，GT09200ZB(F006)，2019/01/01 至 2020/12/30,RMB$148,000，主持</li>	
		    <li style="text-align:justify; text-justify:inter-ideograph;">国家自然科学基金（青年基金），多RGB-D传感器联合的在线室内高精度三维测图方法<a href="https://www.kdocs.cn/l/cmXKsbv0iA4x"> ⏬</a>，41801392，2019/01/01 至 2021/12/31, RMB$265,000，主持</li>
		    <li style="text-align:justify; text-justify:inter-ideograph;">中国博士后科学基金，多元特征混合优化的RGB-D室内高精度三维测图方法<a href="https://www.kdocs.cn/l/ctaxJi6IMHix"> ⏬</a>，2018M633133，2018/05/01 至 2019/09/08,RMB$500,00，主持</li>	
                    <li style="text-align:justify; text-justify:inter-ideograph;">深圳市科创委自由探索项目，基于便携式深度传感器的城市封闭/半封闭空间快速三维测图技术研究<a href="https://www.kdocs.cn/l/cvvGsZtUzOp1"> ⏬</a>，JCYJ20180305125131482，2019/01/01 至 2021/12/31, RMB$300,000，主持</li>
                    <li style="text-align:justify; text-justify:inter-ideograph;">自然资源部城市自然资源监测与仿真重点实验室开放基金, 基于室内高精度三维测图的BIM关键部件自动化重建方法<a href="https://www.kdocs.cn/l/cgONXslJ2kOX"> ⏬</a>，KF-2019-04-010, 2020/01/17 至 2021/12/30，RMB$200,000，主持</li>
                     <li style="text-align:justify; text-justify:inter-ideograph;">武汉大学测绘遥感信息工程国家重点实验室开放基金，集成视觉与几何特征的RGB-D SLAM方法<a href="https://www.kdocs.cn/l/cuQhSejnhoae"> ⏬</a>，17E04，2018/01/01 至 2019/12/31, RMB$500,00，主持</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
	</section>
	<section id="awards">
	 <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards</heading>
              <p>
                <ul>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">2023年度自然资源科学技术奖，实景三维城市环境智能构建关键技术及应用<a href="https://www.kdocs.cn/l/clahjfiJ5evN"> ⏬</a><a href="https://www.kdocs.cn/l/ccAbSixhZIAV"> ⏬</a><a href="https://kdocs.cn/l/chRsedsKcOJA"> ⏬</a>，一等奖，排名：2/15，2024。</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">论文《Accurate Segmentation Method for Roadside Lampposts Based onVehicle.Mounted Lidar Point Clouds》获得ISPRS TC1 Symposium Intelligent Sensing and Remote Sensing Application 国际会议最佳论文奖<a href="https://www.kdocs.cn/l/cqCJiW37FGXU"> ⏬</a>, 2024.</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">发表在《武汉大学学报(信息科学版)》论文《超体素随机森林与LSTM神经网络联合优化的室内点云高精度分类方法》<a href="https://www.kdocs.cn/l/csiEDhO5FMJX"> ⏬</a>，入选2023年度优秀论文, 2024.</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">论文“Robust Calibration of Vehicle Solid-state Lidar-Camera Perception System Using Line-Weighted Correspondences in Natural Environments”获得2023非暴露空间国际学术论坛最佳论文奖<a href="https://www.kdocs.cn/l/ct96kcx3XrAK"> ⏬</a>, 2023.</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">论文“BIM generation from 3D point clouds by combining 3D deep learning and improved morphological approach”入选2023年深圳市第三届优秀科技学术论文成果集<a href="https://www.kdocs.cn/l/cn3S7R4TtKPQ"> ⏬</a>。（"BIM generation from 3D point clouds by combining 3D deep learning and improved morphological approach" was selected into the 2023 Shenzhen Third Excellent Science and Technology Academic Papers Collection）, 2023.</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">The Best Workshop Presentation Award at the ISPRS Geospatial Week 2023 International Conference for 'Tree-GPT: Modular Large Language Model Expert System for Forest Remote Sensing Image Understanding and Interactive Analysis'<a href="https://www.kdocs.cn/l/ciFl4cyK7Lp2"> ⏬</a>, 2023.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> The Second Place of the IPIN 2022 Competition ONLINE Track 2 -Camera in the TWELFTH INTERNATIONAL CONFERENCE ONINDOOR POSITIONING AND INDOORNAVIGATION held in BEIJING<a href="https://www.kdocs.cn/l/cdOxoIxlx5OU"> ⏬</a>, CHINA, on 5-7 SEPTEMBER 2022.</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">深圳市高层次人才C类(C-Class High-Level Talents in Shenzhen City)<a href="https://www.kdocs.cn/l/cgykFlMV9z58"> ⏬</a>, 2019.</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">广东省高校科技成果转化大赛电子信息组二等奖<a href="https://www.kdocs.cn/l/cbmMmxPxza3Q"> ⏬</a>，排名2/5，2020。</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">深圳市南山区"2020创业之星大赛"互联网与移动互联网初创组一等奖<a href="https://www.kdocs.cn/l/croPOGOOt7lA"> ⏬</a>，排名：1/3，2020。</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">全国优秀测绘工程奖铜奖<a href="https://www.kdocs.cn/l/csqXmVsJCTdB"> ⏬</a>，排名：9/16，2022。</li>
			<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">深圳大学教学成果奖<a href="https://www.kdocs.cn/l/cbCKqz4b7SlI"> ⏬</a>，一等奖，排名：11/12，2022。</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
	</section>
	<section id="academicservices">
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Service</heading>
              <p> Committee Member:
              </p>
              <p>
                <ul>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">美国摄影测量与遥感学会会刊Photogrametry Engineering and Remote Sensing(PE&RS)期刊，编委,<a href="https://kdocs.cn/l/cosQkRI87iFg"> ⏬</a>, From 2024</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Guest Editor for Remote Sensing, <a href="https://www.mdpi.com/journal/remotesensing/special_issues/RMKV4988H7">Special issue on “Remote Sensing for 2D/3D Mapping”</a>, From 2023</li>
	            <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Guest Editor for IEEE JSTARS, <a href="https://www.grss-ieee.org/wp-content/uploads/2023/07/cfp_High-fidelity-Urban-3D-Modeling-and-Scene-Simulation.pdf">Special issue on “High-fidelity Urban 3D Modeling and Scene Simulation”</a>, From 2023</li>
	            <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"><a href="https://onlinelibrary.wiley.com/page/journal/14779730/homepage/editorialboard.html"> Junior Editor for The Photogrammetry Record </a>, From 2023</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">国际数字地球学会中国虚拟地理环境委员会委员(Member of the China Virtual Geographic Environment Committee of the International Society for Digital Earth), From 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">中国图学学会BIM专委会委员<a href="https://www.kdocs.cn/l/csYwm05JROBO"> ⏬</a>(Member of the BIM Special Committee of the Chinese Society for Graphics), From 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">中国测绘学会位置服务工委会委员<a href="https://www.kdocs.cn/l/cvmo8H3XgvE1"> ⏬</a>(Member of the Location Service Working Committee of the Chinese Society for Surveying and Mapping), From 2022</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;">国际数字地球学会全国激光雷达专业委员会委员<a href="https://www.kdocs.cn/l/cqkJad5oCHcl"> ⏬</a> (Member of the National LiDAR Professional Committee of the International Society for Digital Earth), From 2024</li>            
		<li style="list-style-position:inside;margin:0;padding:0;text-align:left;">Expert of ISO Technical Committee 59, From 2022</li>
                </ul>
              </p>
              <p> Journal Reviewer:
              </p>
              <p>
                <ul>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> ISPRS Journal of Photogrammetry and Remote Sensing</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> The Photogrammetry Record</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> Automation In Construction</li>
                    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> International Journal of Applied Earth Observation and Geoinformation</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> IEEE Transactions on Industrial Informatics</li>
		    <li style="list-style-position:inside;margin:0;padding:0;text-align:left;"> IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2023)</li>
                </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
	</section>
<section id="activites">
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Activites</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/2024zhuhai.png" alt="PontTuset" width="160" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information>汤圣君受邀参加粤港澳高校联盟 2024年青年学者论坛之地球观测与导航分论坛，并在作“空地跨视角协同定位与优化测图研究<a href="https://www.kdocs.cn/l/cmNsFiAV2jdy"> ⏬
			</a> ”的专题报告。 珠海, 2024年11月8日-9日. <br>
              Participants:  Shengjun Tang，
	      
	      <br>
		<p></p>
            </td>
          </tr>
	<tr>
	</tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/2024Lidarweiyuan.jpg" alt="PontTuset" width="160" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information>团队参加第八届全国激光雷达大会，汤圣君老师担任国际数字地球学会激光雷达专业委员会委员，担任室内三维场景理解与结构化重建专题召集人，并在作“室内点云三维场景图智能构建<a href="https://www.kdocs.cn/l/cbisy3G5rGix"> ⏬</a> ”的专题报告。 桂林, 2024年11月1日-4日. <br>
              Participants:  Shengjun Tang, Linfu Xie
	      <br>
		<p></p>
            </td>
          </tr>
	<tr>
	</tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/GISConf2024.png" alt="PontTuset" width="160" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information>团队参加第十九届GIS理论与方法年会，汤圣君老师担任室内构图与应用专题召集人，并作“场景图和多LLM Agent协同的机器智能交互方法<a href="https://www.kdocs.cn/l/clrdCc6mzEz6"> ⏬</a> ”的专题报告。西安, 2024年10月18日-20日. <br>
              Participants:  Shengjun Tang, Weixi Wang, Xiaoming Li, Linfu Xie
	      <br>
		<p></p>
            </td>
          </tr>
	<tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/CERT_LSD_HKreport.png" alt="PontTuset" width="160" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information>Tang Shengjun was invited by the Hong Kong Lands Department and the Hong Kong Institute of Surveyors to deliver an academic report titled "Exploration of Intelligent Construction of Urban Vegetation Digital Twin Scenarios<a href="https://www.kdocs.cn/l/canSXjCPf5kU"> ⏬</a> " in the CONTINUING PROFESSIONAL DEVELOPMENT section. The CONTINUING PROFESSIONAL DEVELOPMENT of the Hong Kong Institute of Surveyors is an important platform for enhancing the skills of Hong Kong surveyors by inviting international peers in the surveying field for professional exchanges. Hongkong, July 23, 2024. <br>
              Participants: Tang Shengjun
	      <br>
		<p></p>
            </td>
          </tr>
	<tr>
		<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/FSDTHK.jpg" alt="PontTuset" width="160" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information>Tang Shengjun attended the debriefing seminar for the "Framework Spatial Data Standards for the Development of CSDI" project, organized by the Hong Kong Lands Department. As a mainland expert, he delivered a speech on "SDI Development and Potential Data Application in Mainland China<a href="https://www.kdocs.cn/l/cdppZhoM73uh"> ⏬</a> ". Hongkong, June 27, 2024. <br>
              Participants: Tang Shengjun
	      <br>
		<p></p>
            </td>
          </tr>
	<tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/bestpaperisprs.png" alt="PontTuset" width="160" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information>Shengjun Tang, Renzhong Guo, Weixi Wang, Xiaoming Li, Linfu Xie, Yaoyu Li, Zhuoyu Ao and Song Ziyu attended the ISPRS International Society for Photogrammetry and Remote Sensing Conference held in Changsha. The team's paper, "Accurate Segmentation Method for Roadside Lampposts Based on Vehicle Mounted Lidar Point Clouds" won the Best Paper Award. Changsha, May 13-17, 2024. <br>
              Participants: Shengjun Tang, Weixi Wang, Xiaoming Li, Linfu Xie, Yaoyu Li, Zhuoyu Ao, Song Ziyu
	      <br>
		<p></p>
            </td>
          </tr>
	<tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/2023nonex.png" alt="PontTuset" width="160" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information>汤圣君参加在香港举办的2023非暴露空间国际学术论坛，团队论文“Robust Calibration of Vehicle Solid-state Lidar-Camera Perception System Using Line-Weighted Correspondences in Natural Environments”获得2023非暴露空间国际学术论坛最佳论文奖，香港，2023年12月09日（Tang Shengjun and Huang Hongsheng attended the 2023 Non-Exposed Space International Academic Forum held in Hong Kong, where their team's paper "Robust Calibration of Vehicle Solid-state Lidar-Camera Perception System Using Line-Weighted Correspondences in Natural Environments" won the Best Paper Award at the forum, HongKong, in Dec 09th, 2023.）</Conference information>
              <br>
              Participants: Shengjun Tang
	      <br>
		<p></p>
            </td>
          </tr>
	<tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/beijingkongtian.png" alt="PontTuset" width="100" valign="middle" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information>汤圣君在中国科学院空天技术研究院就“面向数字孪生的城市场景感知与建模关键技术研究<a href="https://www.kdocs.cn/l/cuHnLIe2l2Dy"> ⏬</a> ”进行了专题学术报告，北京，2023年12月05日（Tang Shengjun delivered a lecture on "Urban Scene Perception and Modeling Technologies for Digital Twinning" at the Aerospace Technology Research Institute of the Chinese Academy of Sciences, Beijing, in Dec 05th, 2023.）</Conference information>
              <br>
              Participants: Shengjun Tang
	      <br>
		<p></p>
            </td>
          </tr>
	<tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/shenzhenpaper.png" alt="PontTuset" width="160" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information> 团队论文“BIM generation from 3D point clouds by combining 3D deep learning and improved morphological approach”入选2023年深圳市第三届优秀科技学术论文成果集，汤圣君作为代表参加2023年深圳市第三届优秀科技学术论文成果颁奖典礼，深圳，2023年11月26日（The team's paper 'BIM Generation from 3D Point Clouds by Combining 3D Deep Learning and Improved Morphological Approach' has been selected for the collection of the 3rd Shenzhen Outstanding Scientific and Academic Paper Achievements in 2023. Tang Shengjun, as a representative, will attend the award ceremony of the 3rd Shenzhen Outstanding Scientific and Academic Paper Achievements in 2023, Shenzhen, in Nov 26th, 2023.）</Conference information>
              <br>
              Participants: Shengjun Tang
	      <br>
		<p></p>
            </td>
          </tr>
	<tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/shanghaicof.png" alt="PontTuset" width="160" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information> 团队参加“摄影测量与遥感2023学术年会”，并做面向植被要素实景建模的大规模三维仿真树木数据集及应用报告<a href="https://www.kdocs.cn/l/csTsMPs2oqAl"> ⏬</a> ，上海，2023年10月20-22日（ Our team participated in the "Photogrammetry and Remote Sensing 2023 Academic Conference" and presented a large-scale three-dimensional simulation tree dataset and its applications for real-world modeling of vegetation elements, in Shanghai from October 20th to 22nd, 2023.）</Conference information>
              <br>
              Participants: Shengjun Tang, Weixi Wang, Linfu Xie
	      <br>
              Presentations : <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/Ee85cbVqRvZBtQuMk1euW2kBz-Rt8fBn86G5S88tgrC7Mw?e=5f0Avf">面向植被要素实景建模的大规模三维仿真树木数据集及应用（A large-scale three-dimensional simulation tree dataset and its applications for real-world modeling of vegetation elements）  </a> 
              <p></p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/lidarcof.png" alt="PontTuset" width="160" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information> 汤圣君作为室内三维场景理解与结构化重建专题主席参加“第七届全国激光雷达大会”，并做室内高精度视觉位姿估计与AR导航的报告<a href="https://www.kdocs.cn/l/csf3RDqrFicv"> ⏬</a> ,2023年10月20-22日（Tang Shengjun served as the special session chair for indoor 3D scene understanding and structured reconstruction at the "7th National LiDAR Conference". He delivered a report on high-precision indoor visual pose estimation and AR navigation from October 20 to 22, 2023.）</Conference information>
              <br>
              Participants: Shengjun Tang
	      <br>
              Presentations : <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EfUd90HK-6RNsk0WuVqX5MYBBzZjT4GitG2HIWySinMvKA?e=rP1Ss0">室内高精度视觉位姿估计与AR导航（High-precision indoor visual pose estimation and AR navigation）  </a> 
              <p></p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/GSW2023.png" alt="PontTuset" width="160" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information> ISPRS Geospatial Week 2023, is held in Cairo, Egypt from September 2-7, 2023.</Conference information>
              <br>
              Participants: Shengjun Tang, Weixi Wang, Xiaoming Li, Linfu Xie, Siqi Du, Hongsheng Huang
	      <br>
              Presentations : <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/Ebo7ZRcMkTtNtDzdI1SbgXkB6xMfxKutIzxLtOMLP-vVEQ?e=1JeYlc">  Benchmark of synthetic indoor scenes </a> |  <a href="https://szueducn-my.sharepoint.com/:p:/g/personal/shengjuntang_szu_edu_cn/EfS0IvsLPwNGktjfu6QkA5IBTvmD1oqtTDqXojbK5_zu-g?e=ZwFbWw"> TreeGPT </a> 
              <p></p>
            </td>
          </tr>
	<tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="data/yanjicof.png" alt="PontTuset" width="160" style="border-style: none">
	      <meta name="referrer" content="no-referrer" />
            </td>
            <td width="75%" valign="middle">
                <Conference information> 团队2023年8月10日-11日参加第九届全国虚拟地理环境大会，作为高逼真虚拟城市环境构建与动态仿真专题召集人并且作报告<a href=“https://www.kdocs.cn/l/cvPvc5os5PmI”> ⏬</a> 。同时指导团队研究生获得第一届中国虚拟地理环境创新大赛技术探索类特等奖，题目为“城市高逼真数字孪生场景构建关键技术探索”，延吉，2023年08月10-11日。Our team attended the 9th National Virtual Geographic Environment Conference from August 10th to 11th, 2023. As the convener for the topic of constructing high-fidelity virtual urban environments and dynamic simulation, he also presented a report titled "Research on High-Precision 3D Scene Perception and Semantic Understanding." In addition, he mentored a team of graduate students who won the grand prize in the technical exploration category at the first China Virtual Geographic Environment Innovation Competition, with their project "Exploration of Key Technologies for the Construction of High-Fidelity Digital Twin Scenes in Urban Areas."</Conference information>
              <br>
              Participants: Shengjun Tang, Weixi Wang, Linfu Xie
	      <br>
              Presentations : <a href="https://szueducn-my.sharepoint.com/:b:/g/personal/shengjuntang_szu_edu_cn/EZCFazWMxVtEqOa1gB0OKB8BYidTl6yJIf0W9PfBF-ViLQ?e=ggWkw5">高精度三维场景感知与语义理解研究 </a> 
              <p></p>
            </td>
          </tr>
		</tbody></table>
	</section>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>			
        </tbody></table>
        <p align="center">
          <font size="2">
            Template from <a href="https://shiyujiao.github.io/">shiyu jiao's website</a>.
	 </font>
        </p>
      </td>
    </tr>
  </table>
</body>
</html>
